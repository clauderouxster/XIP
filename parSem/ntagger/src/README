1-Data files
	
	Data files are plain text files containing sequence separated by  empty
	lines.  Each  sequence  is a set of non-empty lines where each of these
	represents one position in the sequence.

	Each lines are made of token and label separated either by spaces or by  tabula-
	tions.
	you can find the annotated data files in :"oct27.splits", "tweets_PTB" and "NPS-Chat" 
	folders.
	
	oct27.splits:
			-oct27-modif.train
			-oct27-modif.dev
			-oct27-modif.test
			-daily547-modif

	tweets_PTB:
			-PTB (whole data)
			-train
			-test
			-dev


	NPS-Chat:
			-chats
		
	
	
2-Create Brwon cluster file

	-Download implementation of the Brown clustering algorithm :
			https://github.com/percyliang/brown-cluster
	-Compile
		make clean
		make
	-Run
		Input: a sequence of words separated by whitespcae.
		Output: for each word type, its cluster.In particular, each line is:
	  	<cluster represented as a bit string> <word> <number of times word occurs in input>
		For example in order to cluster input.txt into 50 clusters:
	
	  	./wcluster --text input.txt --c 50 
	 	(Output in input-c50-p1.out/paths)

	Note: The output must be named as "brownCluster.txt", because in code this name has been used.


3-Create MKCLS file

	-Download implementation of the MKCLS algorithm:
		http://www-i6.informatik.rwth-aachen.de/Colleagues/och/software/GIZA++.html
	
	-Compile
	
	-Run
		 mkcls [-nnum] [-ptrain] [-Vfile] opt

		-V output classes
		-n number of optimization runs (Default: 1); larger number => better results
		-p filename of training corpus (Default: 'train')

		Example:
		./mkcls -c80 -n10 -pkorpus -Vkats opt
		(generates 80 classes for the corpus 'in' and writes the classes in 'out') 
	 		
	Note: The output must be named as "mkcls", because in code this name has been used.

4-Extract featured for data set

	In order to train and test, we have to extract feature for all data files. These features are clustering 
	features and some lexical and orthographical features.
	
	-Call "crfInputFile" from tagger.cpp class. This method requires input file path and output file path.

		crfInputFile([input file path],[output file path]) 

5-Prepare Training, Development and Test data set
	
	In order to train your model, you need to devide your data set to training, development and test set.
	They could comes from random sampling or other procedure.

6-Pattern file

	Pattern file are almost compatible with CRF++ templates.  Empty  lines
	as  well  as  all  characters  appearing after a '#' are discarded. The
	remaining lines are interpreted as patterns.

       	The first char must be either 'u', 'b' or '*' (in upper or lower case).
      	This  indicates  the type of feature: respectively unigram, bigrams and
      	both, must be generated from this pattern.(for more detail see http://wapiti.limsi.fr/manual.html)
	For our model the pattern file is in:
		
		 /opt/ParSem/users/Farhad/parSem/ntagger/src/oct27.splits/pattern


7-Train the model (Model File)

8-Measure performance of the model

9-Configure the code in order to tag new instance	


